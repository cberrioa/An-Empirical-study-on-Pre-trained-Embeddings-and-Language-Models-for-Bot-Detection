{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generate human and bots tweets dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cberrioa/An-Empirical-study-on-Pre-trained-Embeddings-and-Language-Models-for-Bot-Detection/blob/master/dataset%20generation/Generate_human_and_bots_tweets_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hbg8VoPLn6o",
        "colab_type": "text"
      },
      "source": [
        "# Connect to Google Drive to access the files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWDHUbL8LSRu",
        "colab_type": "code",
        "outputId": "321f5fea-16a9-4b8a-96ec-52bd2a057c8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbIuYD-0LUMN",
        "colab_type": "text"
      },
      "source": [
        "## Read and prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3a1QlvEMLR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evU6AP0TMSIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_file_tweets_humans=\"/drive/My Drive/app/data/evaluation/tweets-humans.csv\"\n",
        "dataset_file_tweets_bots=\"/drive/My Drive/app/data/evaluation/tweets-bots.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJWr_rBVLWvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(dataset_file_tweets_humans, \"r\", encoding='utf-8', errors=\"surrogatepass\") as file:\n",
        "  dataset_tweets_humans = pd.read_csv(file)\n",
        "  \n",
        "dataset_tweets_humans['label'] = 0\n",
        "\n",
        "with open(dataset_file_tweets_bots, \"r\", encoding='utf-8', errors=\"surrogatepass\") as file:\n",
        "  dataset_tweets_bots = pd.read_csv(file)\n",
        "  \n",
        "dataset_tweets_bots['label'] = 1\n",
        "\n",
        "dataset_complete = dataset_tweets_humans.append(dataset_tweets_bots,ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7ZEcJPnNajs",
        "colab_type": "text"
      },
      "source": [
        "This dataset contains users that appear in both categories, so we delete them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz5N9mACMFzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "intersect = np.intersect1d(dataset_tweets_humans.groupby('user').size().index.values,dataset_tweets_bots.groupby('user').size().index.values)\n",
        "\n",
        "for user in intersect:\n",
        "  dataset_complete = dataset_complete[dataset_complete['user'] != user]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEOiRH1rOqRk",
        "colab_type": "text"
      },
      "source": [
        "Now we are going to take a random sample of 500000 tweets for training and 100000 tweets for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu05eLClOool",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Take a random sample of 500000 tweets for training set\n",
        "dataset_train = dataset_complete.sample(n=500000)\n",
        "\n",
        "# tweets not in training set\n",
        "dataset_complement = dataset_complete.loc[dataset_complete.index.difference(dataset_train.index),:]\n",
        "\n",
        "# take a random sample of 100k in the complementary set\n",
        "dataset_test = dataset_complement.sample(n=100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpP3YPAkQvHn",
        "colab_type": "text"
      },
      "source": [
        "We can save our training and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buRiLDNsQK1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_train.to_csv('/drive/My Drive/app/tweets-humans-and-bots-train.csv')\n",
        "dataset_test.to_csv('/drive/My Drive/app/tweets-humans-and-bots-test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S21Aymr8SGWg",
        "colab_type": "text"
      },
      "source": [
        "Now we have generated our training and test datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH-q2h1u2t5A",
        "colab_type": "text"
      },
      "source": [
        "# Read and prepare the preprocessed dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaFyccgh2-zM",
        "colab_type": "text"
      },
      "source": [
        "If you want to create the same dataset, but with preprocessed tweets, we can use the dataset_train and dataset_test indexes, but using the complete dataset of preprocessed tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z7ambvSB2jNL",
        "colab": {}
      },
      "source": [
        "dataset_file_tweets_humans=\"/drive/My Drive/app/data/evaluation/tweets-humans-preprocessed.csv\"\n",
        "dataset_file_tweets_bots=\"/drive/My Drive/app/data/evaluation/tweets-bots-preprocessed.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "neTNqpb82jNy",
        "colab": {}
      },
      "source": [
        "with open(dataset_file_tweets_humans, \"r\", encoding='utf-8', errors=\"surrogatepass\") as file:\n",
        "  dataset_tweets_humans = pd.read_csv(file)\n",
        "  \n",
        "dataset_tweets_humans['label'] = 0\n",
        "\n",
        "with open(dataset_file_tweets_bots, \"r\", encoding='utf-8', errors=\"surrogatepass\") as file:\n",
        "  dataset_tweets_bots = pd.read_csv(file)\n",
        "  \n",
        "dataset_tweets_bots['label'] = 1\n",
        "\n",
        "dataset_complete = dataset_tweets_humans.append(dataset_tweets_bots,ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB9rUkJuzgoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_train_preprocessed = dataset_complete.loc[dataset_train.index]\n",
        "dataset_test_preprocessed = dataset_complete.loc[dataset_test.index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tje-azzQ4yUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_train_preprocessed.to_csv('/drive/My Drive/app/tweets-humans-and-bots-preprocessed-train.csv')\n",
        "dataset_test_preprocessed.to_csv('/drive/My Drive/app/tweets-humans-and-bots-preprocessed-test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}